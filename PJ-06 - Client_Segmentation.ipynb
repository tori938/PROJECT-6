{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwDgzYzmULe-"
   },
   "source": [
    "# <center> Сегментация Клиентов Онлайн Магазина Подарков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzoQSxWiULe_"
   },
   "source": [
    "## Постановка Задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMaTS3hUULe_"
   },
   "source": [
    "Маркетинг — неотъемлемая часть любого бизнеса. Для повышения прибыли компании важно понимать своего клиента, его пожелания и предпочтения. С появлением электронной коммерции, или онлайн-продаж, стало намного проще собирать данные о клиентах, анализировать их, находить закономерности и реализовывать маркетинговые кампании.\n",
    "\n",
    "Большинство интернет-магазинов используют инструменты веб-аналитики, чтобы отслеживать просмотры страниц, количество и поведение посетителей и коэффициент отказов. Но отчёта из Google Analytics или аналогичной системы может быть недостаточно для полного понимания того, как клиенты взаимодействуют с сайтом. Компаниям важно иметь возможность быстро и точно реагировать на перемены в поведении клиентов, создавая инструменты, которые обнаруживают эти изменения практически в режиме реального времени.\n",
    "\n",
    "Машинное обучение помогает поисковой системе анализировать огромное количество данных о посетителях платформы, узнавать модели поведения профессиональных покупателей, определять категорию клиентов (например, лояльные/перспективные/новички/спящие/ушедшие) и выбирать правильную стратегию взаимодействия с ними.\n",
    "\n",
    "Стоит также отметить, что компании, использующие машинное обучение на своих платформах электронной коммерции, могут постоянно повышать эффективность бизнес-процессов: настраивать товарную выборку персонально для каждого покупателя и предлагать выгодную цену в соответствии с бюджетом клиента и т. д. Эта задача относится к категории построения рекомендательных систем, речь о которых пойдёт в следующем разделе нашего курса.\n",
    "\n",
    "> Как правило, наборы данных для электронной коммерции являются частной собственностью и, следовательно, их трудно найти среди общедоступных данных. Однако [The UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php)  создал набор данных, содержащий фактические транзакции за 2010 и 2011 годы. С ним нам как раз и предлагается поработать в этом кейсе. \n",
    "\n",
    "> В нашем распоряжении будет набор данных, который содержит все транзакции, произошедшие в период с 01/12/2010 по 09/12/2011 для базирующейся в Великобритании компании, занимающейся онлайн-розничной торговлей. Компания в основном продает уникальные подарки на все случаи жизни. Многие клиенты компании являются оптовиками.\n",
    "\n",
    "\n",
    "**Бизнес-задача:** произвести сегментацию существующих клиентов, проинтерпретировать эти сегменты и определить стратегию взаимодействия с ними.\n",
    "\n",
    "**Техническая задача для вас как для специалиста в Data Science:** построить модель кластеризации клиентов на основе их покупательской способности, частоты заказов и срока давности последней покупки, определить профиль каждого из кластеров.\n",
    "\n",
    "**Основные цели проекта:**\n",
    "1. Произвести предобработку исходного набора данных о транзакциях.\n",
    "2. Провести разведывательный анализ данных и выявить основные закономерности.\n",
    "3. Сформировать набор данных о характеристиках каждого из уникальных клиентов.\n",
    "4. Построить несколько моделей машинного обучения, решающих задачу кластеризации клиентов, определить количество кластеров и проинтерпретировать их.\n",
    "5. Спроектировать процесс предсказания категории интересов клиента и протестировать вашу модель на новых клиентах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqd34ABEULe_"
   },
   "source": [
    "## Описание Данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RftXwcHIULe_"
   },
   "source": [
    "Данные представляют собой таблицу в формате CSV, в каждой строке которой содержится информация об уникальной транзакции.\n",
    "\n",
    "Признаки, описывающие каждую транзакцию:\n",
    "\n",
    "* InvoiceNo — номер счёта-фактуры (уникальный номинальный шестизначный номер, присваиваемый каждой транзакции; буква \"C\" в начале кода указывает на отмену транзакции);\n",
    "* StockCode — код товара (уникальное пятизначное целое число, присваиваемое каждому отдельному товару);\n",
    "* Description — название товара;\n",
    "* Quantity — количество каждого товара за транзакцию;\n",
    "* InvoiceDate — дата и время выставления счёта/проведения транзакции;\n",
    "* UnitPrice — цена за единицу товара в фунтах стерлингов;\n",
    "* CustomerID — идентификатор клиента (уникальный пятизначный номер, однозначно присваиваемый каждому клиенту);\n",
    "* Country — название страны, в которой проживает клиент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BmfO_op3ULe_"
   },
   "outputs": [],
   "source": [
    "#standard libraries\n",
    "import pandas as pd, numpy as np\n",
    "import datetime\n",
    "\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "#plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "#store results\n",
    "#from collections import OrderedDict\n",
    "\n",
    "\n",
    "#warnings\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#data transformation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#clusterization\n",
    "from sklearn import cluster\n",
    "from sklearn import mixture\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "\n",
    "#classification\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "\n",
    "#quality control\n",
    "from sklearn import metrics\n",
    "\n",
    "#model learning on train / test\n",
    "from sklearn import model_selection\n",
    "\n",
    "#user-made\n",
    "from user_fx import get_quantity_cancelled\n",
    "from user_fx import plot_cluster_profile\n",
    "from user_fx import clusters_by_silhouette, clusters_by_calinski_harabasz, clusters_by_davies_bouldin\n",
    "from user_fx import best_result_by_silhouette, best_result_by_calinski_harabasz, best_result_by_davies_bouldin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKmK361fULe_"
   },
   "source": [
    "### **1. Знакомство со Структурой Данных**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ss2zvT4bULe_"
   },
   "source": [
    "Первым делом необходимо понять, с какими данными предстоит работать, и произвести базовую предобработку данных — перевести признаки в необходимые для дальнейшей работы форматы.\n",
    "\n",
    "Познакомьтесь с исходными данными поближе:\n",
    "\n",
    "* Проведите статистический анализ исходных данных, посмотрев на основные диапазоны исходных признаков.\n",
    "* Узнайте, сколько уникальных клиентов совершали транзакции в указанный период.\n",
    "* Узнайте, из каких стран совершались транзакции.\n",
    "* Исследуйте данные на наличие пропусков и дубликатов.\n",
    "* Переведите столбцы в корректные форматы (например, даты в формат datetime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_VKwxlvULfA",
    "outputId": "0b8e41ed-a5b6-4ae9-f589-94d51295eb7f"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/data.csv',\n",
    "                   encoding='ISO-8859-1',\n",
    "                   dtype={'CustomerID': str,\n",
    "                          'InvoiceID': str}\n",
    ")\n",
    "\n",
    "f'Dimensions: {data.shape}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AeKmc494ULfB",
    "outputId": "e4965d30-2459-4584-cc9a-2aec725bbe1b"
   },
   "outputs": [],
   "source": [
    "#convert to datetime\n",
    "data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'])\n",
    "\n",
    "print(f\"Date Interval: from {data['InvoiceDate'].dt.date.min()} to {data['InvoiceDate'].dt.date.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baUWwPFjULfB"
   },
   "source": [
    "### **2. Преобразование, Очистка и Анализ Данных**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-djEuu-ULfB"
   },
   "source": [
    "#### **2.1. Преобразование и Очистка данных о Транзакциях**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYFcbGdGULfB"
   },
   "source": [
    "##### 2.1.1 Пропуски\n",
    "\n",
    "Пропуски в столбце с идентификатором клиента (CustomerID) и описанием товара свидетельствуют о некорректных/незавершённых транзакциях. Удалите их из данных.\n",
    "\n",
    "**Примечание.** Если посмотреть на распределение пропусков в столбцах Description и CustomerID, то можно заметить, что достаточно удалить строки, содержащие пропуски в столбце CustomerID, тогда пропуски в столбце Description удаляются автоматически.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Io-D4T80UTyB"
   },
   "outputs": [],
   "source": [
    "#check that there are no null values in the data set\n",
    "data.isnull().sum()[data.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete all records with any blanks\n",
    "data = data.dropna(axis=0,\n",
    "                   how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that there are no null values in the data set\n",
    "data.isnull().sum()[data.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'New Dimensions: {data.shape}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFmgQYkkULfB"
   },
   "source": [
    "##### 2.1.2. Дубликаты\n",
    "\n",
    "Проверьте данные на наличие дубликатов. Удалите их из данных.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYg8iXKsUUaH"
   },
   "outputs": [],
   "source": [
    "dupl_columns = list(data.columns)\n",
    "\n",
    "d_mask = data.duplicated(subset=dupl_columns)\n",
    "d_duplicates = data[d_mask]\n",
    "print(f'Number of Duplicates: {d_duplicates.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new table free of duplicates\n",
    "data = data.drop_duplicates(subset=dupl_columns)\n",
    "print(f'New Dimensions without Duplicates: {data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBRPST-4ULfB"
   },
   "source": [
    "##### 2.1.3. Транзакции с отрицательным количеством товара\n",
    "\n",
    "Из приведённых выше инсайтов о данных мы узнали все особенности отмены заказов. Когда мы будем сегментировать клиентов, мы будем определять их покупательскую способность. При расчёте покупательской способности нам будет очень важно учесть возвраты этого клиента.\n",
    "\n",
    "Чтобы подсчитать количество возвратов, для начала нам надо определить, сколько уникальных товаров указано в транзакции (корзине) для каждой уникальной пары «клиент — заказ»:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3aVwryCULfC",
    "outputId": "4621f283-6f69-4235-93f7-156fab893552"
   },
   "outputs": [],
   "source": [
    "temp = data.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate'].count()\n",
    "\n",
    "nb_products_per_basket = temp.rename(columns = {'InvoiceDate': 'Number of Products'})\n",
    "nb_products_per_basket.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1tfT1K9ULfC"
   },
   "source": [
    "**Примечание.** Более 16 % уникальных заказов являются возвратами. Интересный факт: если мы подсчитали количество транзакций, содержащих признак возврата, в изначальной таблице, где на каждый уникальный товар заведена отдельная строка, то мы получили бы, что количество возвратов менее 1 %. Однако это число было бы некорректным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJHQS1T3ULfC"
   },
   "source": [
    "Создайте в ваших данных о транзакциях признак `QuantityCanceled`. Этот признак будет указывать на количество отменённого впоследствии товара для каждой транзакции, на которую найдётся противоположная ей транзакция с возвратом. Для транзакций, для которых не было возвратов, признак будет равен 0. Для транзакций, для которых указано отрицательное количество товара, но в данных отсутствует контрагент, признак будет пустым.\n",
    "\n",
    "В качестве вспомогательного инструмента мы подготовили для вас функцию `get_quantity_canceled()`. Эта функция принимает на вход таблицу с транзакциями и возвращает объект `Series` — столбец, в котором указано количество отменённого впоследствии товара для каждой транзакции. Если транзакция не имеет контрагента, этот признак помечается как `NaN`.\n",
    "\n",
    "Отметим, что эта функция не учитывает сложный случай, когда количество отменённого товара в транзакции-возврате больше, чем количество товара, которое указано в любой из отдельных транзакций на покупку (это случай, когда клиент сделал несколько заказов одного и того же товара, а потом оформил возврат на все товары разом). При желании вы можете самостоятельно модифицировать функцию для предобработки, чтобы учесть этот пограничный случай."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GBLHrH2eULfC"
   },
   "outputs": [],
   "source": [
    "data['QuantityCanceled'] = get_quantity_cancelled(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qq9sOiKRULfC"
   },
   "source": [
    "Узнайте, сколько транзакций в данных не имеют контрагентов, и, если их количество невелико, удалите их из данных.\n",
    "\n",
    "Когда вы разобрались с возвратами, удалите транзакции с отрицательным количеством товара — они нам больше не нужны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4KThorYUaY5"
   },
   "outputs": [],
   "source": [
    "print(f'{data.QuantityCanceled.isnull().sum()} transactions have no counterparty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete all transactions without a counterparty\n",
    "data = data.dropna(axis=0,\n",
    "                   how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_index = []\n",
    "for index, col in data.iterrows():\n",
    "    if col['Quantity'] - col['QuantityCanceled'] == 0:\n",
    "        list_index.append(index)\n",
    "    if col['Quantity'] < 0:\n",
    "        list_index.append(index)\n",
    "\n",
    "data = data.drop(index=list_index,\n",
    "                 axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'New Dimensions: {data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnuyzW41ULfC"
   },
   "source": [
    "##### 2.1.4. Специализированные транзакции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ww6xIVQULfC"
   },
   "source": [
    "Следующая задача — обработать специальные виды транзакций, которые обозначаются латинскими буквами.\n",
    "\n",
    "В данных присутствует шесть специальных видов транзакций. С помощью регулярных выражений найдите такие коды товаров (StockCode), которые начинаются с латинских букв (при этом коды могут содержать цифры).\n",
    "\n",
    "**Подсказка.** В качестве шаблона для поиска используйте строку '^[a-zA-Z]+'.\n",
    "\n",
    "Чтобы понять, что означают эти коды, можно заглянуть в столбец с описанием (Description), например POST означает почтовые расходы, C2 — расходы на транспортировку, BANK CHARGES — банковские расходы.\n",
    "\n",
    "Специальные операции не характеризуют покупательскую способность клиентов, так как не относятся напрямую к их покупкам, поэтому такие записи нам не нужны. Удалите все специальные транзакции из таблицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUy0AbbPUY88"
   },
   "outputs": [],
   "source": [
    "#search for special transactions\n",
    "sp_char = '^[a-zA-Z]+'\n",
    "\n",
    "special_trans = data.loc[data['StockCode'].str.contains(sp_char,\n",
    "                                                        regex=True)]\n",
    "\n",
    "special_trans['StockCode'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the index of special transactions\n",
    "sp_idx = data.loc[data['StockCode'].str.contains(sp_char,\n",
    "                                                 regex=True)].index\n",
    "\n",
    "data = data.drop(index=sp_idx,\n",
    "                 axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'New Dimensions: {data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESJ2tqOyULfD"
   },
   "source": [
    "##### 2.1.5. Транзакции с товарами без стоимости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0mG0DaQULfD"
   },
   "source": [
    "При просмотре описательных статистик можно заметить, что на некоторые товары установлена цена в 0 фунтов стерлингов. Таких транзакций оказывается менее 1 % — можно удалить их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gicL3AL8Ub-g"
   },
   "outputs": [],
   "source": [
    "zero_price = data[data['UnitPrice'] == 0].index\n",
    "\n",
    "data = data.drop(index=zero_price,\n",
    "                 axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCNmsg22ULfD"
   },
   "source": [
    "##### 2.1.6. Общая стоимость товаров в транзакции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKzMOraCULfD"
   },
   "source": [
    "Добавьте в ваш датасет общую цену заказа (TotalPrice). Она рассчитывается как:\n",
    " \n",
    " **общая цена = цена за единицу товара * (количество товаров в заказе - количество возвращённых товаров).**\n",
    "\n",
    "Этот признак впоследствии поможет вам рассчитать покупательскую способность каждого из клиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dFSeCsQiUc1P"
   },
   "outputs": [],
   "source": [
    "data['TotalPrice'] = pd.Series(np.zeros(data.shape[0]),\n",
    "                               index=data.index)\n",
    "\n",
    "for index, col in data.iterrows():\n",
    "    data['TotalPrice'].loc[index] = col['UnitPrice'] * (col['Quantity'] - col['QuantityCanceled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the table\n",
    "data.to_csv('data/transformed.csv',\n",
    "            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('data/transformed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dimensions: {new_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsbpZiUVULfD"
   },
   "source": [
    "Помимо рекомендованных преобразований, вы также можете придумать собственные и применить их к исходным данным.\n",
    "После завершения предобработки сохраните результат очищения данных в отдельный файл, чтобы впоследствии вам не приходилось повторять эти действия.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4U5l2mQULfD"
   },
   "source": [
    "#### **2.2. Разведывательный Анализ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9p4TuiMULfE"
   },
   "source": [
    "После предобработки исходных данных произведите разведывательный анализ и исследуйте транзакции, ответив на следующие вопросы:\n",
    "\n",
    "* Клиенты из каких стран покупают больше и чаще?\n",
    "* Какие страны приносят наибольшую сезонную выручку?\n",
    "* Присутствует ли в продажах сезонность (когда покупают чаще)?\n",
    "* Сгруппируйте данные по датам и часам совершения транзакции и найдите количество заказов на каждый день-час. Затем найдите среднее количество ежедневно поступающих заказов в каждый из часов.\n",
    "* Каково распределение среднего количества ежедневно поступающих заказов по времени суток (часу совершения транзакции)?\n",
    "\n",
    "**Примечание.** Вы можете сформулировать и другие вопросы. Главная цель — извлечь максимум понятной информации из исходных данных.\n",
    "\n",
    "Свои рассуждения сопроводите графиками и диаграммами.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Клиенты из каких стран покупают больше и чаще?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xzt673ipULfE"
   },
   "outputs": [],
   "source": [
    "#top 5 countries with top revenue\n",
    "df_total_revenue = pd.DataFrame(new_data.groupby('Country')['TotalPrice'].sum().sort_values(ascending=False)[:5])\n",
    "df_total_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create two charts side by side\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    subplot_titles=('Distribution of Total Revenue by Top 5 Countries', \n",
    "                    'Distribution of Total Revenue by Top 4 Countries')\n",
    "    )\n",
    "\n",
    "\n",
    "#create bar chart\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df_total_revenue.index,\n",
    "    y=df_total_revenue['TotalPrice']),\n",
    "              row=1,\n",
    "              col=1)\n",
    "\n",
    "#create bar chart\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df_total_revenue[1:].index,\n",
    "    y=df_total_revenue[1:]['TotalPrice']),\n",
    "              row=2,\n",
    "              col=1)\n",
    "\n",
    "#set characteristics\n",
    "fig.update_layout(width=1100,\n",
    "                  height=500,\n",
    "                  showlegend=False)\n",
    "\n",
    "#fig.show()\n",
    "\n",
    "#export the graph\n",
    "fig.write_html('plotly_graphs/2.2.1a_bar_total_revenue_by_country.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**: \\\n",
    "Наибольший доход поступает из Великобритании, доход от 2 до 4 других стран-лидеров превышает всего лишь более 200 тыс., в то время как доход Великобритании составляет почти 6,8 млн."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution of revenue by country\n",
    "uk_rev = new_data[new_data['Country'] == 'United Kingdom']\n",
    "netherlands_rev = new_data[new_data['Country'] == 'Netherlands']\n",
    "eire_rev = new_data[new_data['Country'] == 'EIRE']\n",
    "germany_rev = new_data[new_data['Country'] == 'Germany']\n",
    "france_rev = new_data[new_data['Country'] == 'France']\n",
    "\n",
    "#create side-by-side plots\n",
    "fig = make_subplots(rows=5,\n",
    "                    cols=1)\n",
    "\n",
    "#first graph\n",
    "fig.add_trace(\n",
    "    go.Box(x=uk_rev['TotalPrice'], name='United Kingdom'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "#second graph\n",
    "fig.add_trace(\n",
    "    go.Box(x=netherlands_rev['TotalPrice'], name='Netherlands'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "#third graph\n",
    "fig.add_trace(\n",
    "    go.Box(x=eire_rev['TotalPrice'], name='EIRE'),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "#fourth graph\n",
    "fig.add_trace(\n",
    "    go.Box(x=germany_rev['TotalPrice'], name='Germany'),\n",
    "    row=4, col=1\n",
    ")\n",
    "\n",
    "#fifth graph\n",
    "fig.add_trace(\n",
    "    go.Box(x=france_rev['TotalPrice'], name='France'),\n",
    "    row=5, col=1\n",
    ")\n",
    "\n",
    "#set the same axis limits\n",
    "fig.update_xaxes(range=[0, 200])\n",
    "\n",
    "\n",
    "#set characteristics\n",
    "fig.update_layout(height=500,\n",
    "                  width=1100,\n",
    "                  showlegend=False,\n",
    "                  title_text='Distribution of Revenue by Country',\n",
    "                  \n",
    ")\n",
    "\n",
    "#fig.show()\n",
    "\n",
    "#export the graph\n",
    "fig.write_html('plotly_graphs/2.2.1b_box_revenue_by_country.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**: \\\n",
    "Средний доход от одной покупки не превышает отметки 20,0 для трех ведущих стран, а для Великобритании он даже ниже — 10,2.\n",
    "Нидерланды — страна с самым высоким средним доходом и самым большим диапазоном между самой низкой и самой высокой ценой покупки. \\\n",
    "Во всех ведущих странах наблюдается большое количество выбросов, указывающих на разное покупательское поведение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 5 countries with top sales\n",
    "df_total_sales = pd.DataFrame(new_data.groupby('Country').count().sort_values(by='InvoiceNo',\n",
    "                                                                              ascending=False)[:5]).iloc[:, :1]\n",
    "df_total_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create two charts side by side\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    subplot_titles=('Distribution of Total Sales by Top 5 Countries', \n",
    "                    'Distribution of Total Sales by Top 4 Countries')\n",
    "    )\n",
    "\n",
    "#create bar chart\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df_total_sales.index,\n",
    "    y=df_total_sales['InvoiceNo']),\n",
    "              row=1,\n",
    "              col=1)\n",
    "\n",
    "#create bar chart\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df_total_sales[1:].index,\n",
    "    y=df_total_sales[1:]['InvoiceNo']),\n",
    "              row=2,\n",
    "              col=1)\n",
    "\n",
    "#set characteristics\n",
    "fig.update_layout(width=1100,\n",
    "                  height=500,\n",
    "                  showlegend=False)\n",
    "\n",
    "#fig.show()\n",
    "\n",
    "#export the graph\n",
    "fig.write_html('plotly_graphs/2.2.2_bar_total_sales_by_country.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**: \\\n",
    "Наибольшее количество покупок поступает из Великобритании, доход от 2 до 4 других стран-лидеров превышает всего лишь более 7 тыс., в то время как количество покупок Великобритании составляет более 345 тыс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какие страны приносят наибольшую сезонную выручку?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to datetime\n",
    "new_data['InvoiceDate'] = pd.to_datetime(new_data['InvoiceDate'])\n",
    "\n",
    "#create the quarter\n",
    "new_data['Quarter'] = new_data['InvoiceDate'].dt.quarter\n",
    "\n",
    "#get the data for the 1st quarter\n",
    "df_quarter_one = new_data[new_data['Quarter'] == 1]\n",
    "df_quarter_one_rvn = df_quarter_one.groupby(by=['Country', 'Quarter'])['TotalPrice'].sum().sort_values(ascending=False)[:5]\n",
    "df_quarter_one_rvn = df_quarter_one_rvn.unstack().reset_index().rename_axis(None, axis=1)\n",
    "df_quarter_one_rvn = df_quarter_one_rvn.rename(columns={1: 'TotalPrice'})\n",
    "\n",
    "#get the data for the 2nd quarter\n",
    "df_quarter_two = new_data[new_data['Quarter'] == 2]\n",
    "df_quarter_two_rvn = df_quarter_two.groupby(by=['Country', 'Quarter'])['TotalPrice'].sum().sort_values(ascending=False)[:5]\n",
    "df_quarter_two_rvn = df_quarter_two_rvn.unstack().reset_index().rename_axis(None, axis=1)\n",
    "df_quarter_two_rvn = df_quarter_two_rvn.rename(columns={2: 'TotalPrice'})\n",
    "\n",
    "#get the data for the 3rd quarter\n",
    "df_quarter_three = new_data[new_data['Quarter'] == 3]\n",
    "df_quarter_three_rvn = df_quarter_three.groupby(by=['Country', 'Quarter'])['TotalPrice'].sum().sort_values(ascending=False)[:5]\n",
    "df_quarter_three_rvn = df_quarter_three_rvn.unstack().reset_index().rename_axis(None, axis=1)\n",
    "df_quarter_three_rvn = df_quarter_three_rvn.rename(columns={3: 'TotalPrice'})\n",
    "\n",
    "#get the data for the 4th quarter\n",
    "df_quarter_four = new_data[new_data['Quarter'] == 4]\n",
    "df_quarter_four_rvn = df_quarter_four.groupby(by=['Country', 'Quarter'])['TotalPrice'].sum().sort_values(ascending=False)[:5]\n",
    "df_quarter_four_rvn = df_quarter_four_rvn.unstack().reset_index().rename_axis(None, axis=1)\n",
    "df_quarter_four_rvn = df_quarter_four_rvn.rename(columns={4: 'TotalPrice'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create two charts side by side\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    subplot_titles=('Distribution of Quarter 1 Sales by Top 5 Countries', \n",
    "                    'Distribution of Quarter 2 Sales by Top 5 Countries',\n",
    "                    'Distribution of Quarter 3 Sales by Top 5 Countries',\n",
    "                    'Distribution of Quarter 4 Sales by Top 5 Countries'\n",
    "                    )\n",
    "    )\n",
    "\n",
    "\n",
    "#1st graph\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_quarter_one_rvn['TotalPrice'],\n",
    "           y=df_quarter_one_rvn['Country'],\n",
    "           orientation='h'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "#2nd graph\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_quarter_two_rvn['TotalPrice'],\n",
    "           y=df_quarter_two_rvn['Country'],\n",
    "           orientation='h'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "#3rd graph\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_quarter_three_rvn['TotalPrice'],\n",
    "           y=df_quarter_three_rvn['Country'],\n",
    "           orientation='h'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "#4th graph\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_quarter_four_rvn['TotalPrice'],\n",
    "           y=df_quarter_four_rvn['Country'],\n",
    "           orientation='h'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "#set the same axis limits\n",
    "fig.update_xaxes(range=[0, 2550000])\n",
    "\n",
    "#set characteristics\n",
    "fig.update_layout(width=1180,\n",
    "                  height=500,\n",
    "                  showlegend=False)\n",
    "\n",
    "#fig.show()\n",
    "\n",
    "#export the graph\n",
    "fig.write_html('plotly_graphs/2.2.3_bar_total_sales_by_country_by_quarter.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Присутствует ли в продажах сезонность (когда покупают чаще)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**: \\\n",
    "Клиенты из Великобритании демонстрируют кумулятивную модель поведения: каждый квартал имеет более высокие продажи, чем предыдущий; другие страны демонстрируют иные модели поведения, например, у EIRE самые высокие покупки в 3-м квартале. \\\n",
    "Не все страны остаются в первой пятерке в течение всего года, если рассматривать квартал за кварталом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгруппируйте данные по датам и часам совершения транзакции и найдите количество заказов на каждый день-час. Затем найдите среднее количество ежедневно поступающих заказов в каждый из часов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new datetime features: month, day of the month, week day, and hour\n",
    "new_data['Month'] = new_data['InvoiceDate'].dt.month\n",
    "new_data['Day'] = new_data['InvoiceDate'].dt.day\n",
    "new_data['WeekDay'] = new_data['InvoiceDate'].dt.dayofweek\n",
    "new_data['Hour'] = new_data['InvoiceDate'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the sales by month\n",
    "df_monthly_sales = new_data.groupby('Month')['InvoiceNo'].count().round()\n",
    "\n",
    "#create a bar chart\n",
    "fig = px.bar(\n",
    "    data_frame=df_monthly_sales,\n",
    "    x=df_monthly_sales.index,\n",
    "    y='InvoiceNo',\n",
    "    color='InvoiceNo',\n",
    "    width=1100,\n",
    "    height=500,\n",
    "    color_continuous_scale='brwnyl',\n",
    "    title='Distribution of Monthly Sales',\n",
    ")\n",
    "\n",
    "#set the title of the axis and y-axis limits\n",
    "fig.update_layout(\n",
    "    xaxis_title_text='Month',\n",
    "    yaxis_title_text='Number of Sales',\n",
    ")\n",
    "\n",
    "#fig.show()\n",
    "\n",
    "#export the graph\n",
    "fig.write_html('plotly_graphs/2.2.4a_bar_sales_by_month.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**: \\\n",
    "Самые высокие продажи приходятся на 4-й квартал, а самые высокие — на ноябрь, т.е. вероятнее всего, клиенты покупают заранее к Рождеству и Новому Году."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the sales by day of the month\n",
    "df_daily_sales = new_data.groupby('Day')['InvoiceNo'].count().round()\n",
    "\n",
    "#create a bar chart\n",
    "fig = px.bar(\n",
    "    data_frame=df_daily_sales,\n",
    "    x=df_daily_sales.index,\n",
    "    y='InvoiceNo',\n",
    "    color='InvoiceNo',\n",
    "    width=1100,\n",
    "    height=500,\n",
    "    color_discrete_sequence='olive',\n",
    "    title='Distribution of Daily Sales',\n",
    ")\n",
    "\n",
    "#set the title of the axis and y-axis limits\n",
    "fig.update_layout(\n",
    "    xaxis_title_text='Day',\n",
    "    yaxis_title_text='Number of Sales',\n",
    ")\n",
    "\n",
    "#fig.show()\n",
    "\n",
    "#export the graph\n",
    "fig.write_html('plotly_graphs/2.2.4b_bar_sales_by_day.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**: \\\n",
    "Довольно случайное распределение продаж по дням, трудно сделать суждение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the sales by day of the week\n",
    "df_week_sales = new_data.groupby('WeekDay')['InvoiceNo'].count().round()\n",
    "\n",
    "#create a bar chart\n",
    "fig = px.bar(\n",
    "    data_frame=df_week_sales,\n",
    "    x=df_week_sales.index,\n",
    "    y='InvoiceNo',\n",
    "    color='InvoiceNo',\n",
    "    width=1100,\n",
    "    height=500,\n",
    "    color_discrete_sequence='salmon',\n",
    "    title='Distribution of Week Day Sales',\n",
    ")\n",
    "\n",
    "#set the title of the axis and y-axis limits\n",
    "fig.update_layout(\n",
    "    xaxis_title_text='Week Day',\n",
    "    yaxis_title_text='Number of Sales',\n",
    ")\n",
    "\n",
    "#fig.show()\n",
    "\n",
    "#export the graph\n",
    "fig.write_html('plotly_graphs/2.2.4c_bar_sales_by_weekday.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**: \\\n",
    "Большинство покупок совершается в начале недели с понедельника по четверг, растет кумулятивно, с самым высоким показателем в четверг.\n",
    "По данным в субботу не было совершено ни одной покупки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# строим сводную таблицу и вычисляем по ней среднее количество заказов в час\n",
    "hourly_sales = new_data.pivot_table(\n",
    "    values='StockCode',\n",
    "    index='InvoiceDate',\n",
    "    columns='Hour',\n",
    "    aggfunc='count', \n",
    "    fill_value=0\n",
    ").mean().round(2)\n",
    "\n",
    "#create a bar chart\n",
    "fig = px.bar(\n",
    "    data_frame=hourly_sales,\n",
    "    x=hourly_sales.index,\n",
    "    y=hourly_sales.values,\n",
    "    color=hourly_sales.values,\n",
    "    width=1100,\n",
    "    height=500,\n",
    "    color_discrete_sequence='magenda',\n",
    "    title='Distribution of Average Hourly Sales',\n",
    ")\n",
    "\n",
    "#set the title of the axis and y-axis limits\n",
    "fig.update_layout(\n",
    "    xaxis_title_text='Hour',\n",
    "    yaxis_title_text='Number of Average Sales',\n",
    ")\n",
    "\n",
    "#fig.show()\n",
    "\n",
    "#export the graph\n",
    "fig.write_html('plotly_graphs/2.2.4d_bar_average_sales_by_hour.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каково распределение среднего количества ежедневно поступающих заказов по времени суток (часу совершения транзакции)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** \\\n",
    "Наибольшее количество заказов совершается в период с 12.00 до 13.00, т.е., скорее всего, из-за того, что клиенты совершают заказы в обеденный перерыв и имеют свободное время в выходные дни, и то, что среднее количество по часам имеет форму нормального распределения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LujMyZU9ULfE"
   },
   "source": [
    "#### **2.3. Построение RFM-таблицы и поиск RFM-выбросов**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvCn3MBWULfE"
   },
   "source": [
    "Мы добрались до самой интересной части нашей задачи. Нам предстоит сформировать признаки, на основе которых мы будем производить сегментацию клиентов.\n",
    "\n",
    "Для этого давайте познакомимся с очень популярным методом для анализа потребительской ценности под названием RFM. \n",
    "\n",
    "<center> <img src=https://miro.medium.com/max/1400/1*uYQjy9SUjW7iWHc2gGanQQ.png align=\"right\" width=\"400\"/> </center>\n",
    "\n",
    "Метод заключается в группировке клиентов на основе следующих параметров:\n",
    "* Recency (Давность) — давность последней покупки клиента;\n",
    "* Frequency (Частота) — общее количество покупок клиента;\n",
    "* Monetary Value (Денежная ценность) — сколько денег потратил клиент.\n",
    "\n",
    "Суть RFM-анализа состоит в том, что мы разделяем всех клиентов на группы в зависимости от того, как давно они сделали последнюю покупку, как часто покупали и насколько большой была сумма их заказов.\n",
    "\n",
    "Например, вот так может выглядеть интерпретация кластеров для случая RF-сегментации (анализа на основе давности и частоты заказов клиента):\n",
    "\n",
    "<img src=https://retailrocket.ru/wp-content/uploads/2017/06/rfm-1.png>\n",
    "\n",
    "Задача маркетологов — вести клиента в зону лояльных.\n",
    "\n",
    "Мы можем рассчитать RFM-характеристики для каждого из клиентов в нашем датасете и на их основе с помощью методов кластеризации построить подобные сегменты клиентов, привязанные к нашим данным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1eOgmigULfE"
   },
   "source": [
    "Чтобы получить RFM-таблицу, нам необходимо сгруппировать данные по идентификаторам клиента и рассчитать следующие  агрегированные характеристики:\n",
    "\n",
    "* Recency для i-го клиента рассчитывается как разница между датой и временем последнего заказа и точкой отсчёта, переведённая в дни:\n",
    "    $$t_0-max(t_{i1}, t_{i2},..., t_{iM})$$\n",
    "\n",
    "    где $t_{ij}$ — дата и время совершения i-ым клиентом своей j-ой покупки.\n",
    "\n",
    "    В качестве точки отсчёта $t_0$ берём дату на один день «старше», чем все наши данные. Это будет 10 декабря 2011 года (в формате datetime — '2011-12-10 00:00:00').\n",
    "\n",
    "* Frequency рассчитывается как общее количество уникальных заказов, которые совершил i-ый клиент.\n",
    "* Monetary Value рассчитывается как общая сумма денег, которую i-ый клиент потратил на наши товары (с учётом возвратов).\n",
    "\n",
    "Когда вы рассчитаете все характеристики, не забудьте дать столбцам результирующей таблицы соответствующие названия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_data = new_data.groupby('CustomerID')['InvoiceDate'].max().reset_index()\n",
    "\n",
    "recency_data['Recency'] = pd.to_datetime('2011-12-10') - recency_data['InvoiceDate']\n",
    "\n",
    "recency_data['Recency'] = recency_data['Recency'].dt.days\n",
    "\n",
    "rfm_table = pd.DataFrame({\n",
    "    'Recency': recency_data['Recency'].values,\n",
    "    'Frequency': new_data.groupby('CustomerID')['InvoiceNo'].nunique(),\n",
    "    'Monetary': new_data.groupby('CustomerID')['TotalPrice'].sum()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"customers who placed an order more than 200 days ago: {rfm_table[rfm_table['Recency'] > 200].shape[0]}\")\n",
    "print(f\"average number of orders per year: {round(rfm_table['Frequency'].mean())}\")\n",
    "print(f\"purchase amount by customer №12360: £{round(rfm_table.loc[12360]['Monetary'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwFYb5fJULfE"
   },
   "source": [
    "После того как вы подготовите RFM-таблицу, визуализируйте каждую из трёх компонент, например, в виде коробчатых диаграмм (boxplot). Вы получите примерно следующие результаты (графики могут отличаться в зависимости от того, как вы выполните предобработку данных):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create side-by-side plots\n",
    "fig = make_subplots(rows=1,\n",
    "                    cols=3)\n",
    "\n",
    "#first graph: recency\n",
    "fig.add_trace(\n",
    "    go.Box(x=rfm_table.iloc[:, 0].values,\n",
    "           name='Recency'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "#second graph: frequency\n",
    "fig.add_trace(\n",
    "    go.Box(x=rfm_table.iloc[:, 1].values,\n",
    "           name='Frequency'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "#third graph: monetary\n",
    "fig.add_trace(\n",
    "    go.Box(x=rfm_table.iloc[:, 2].values,\n",
    "           name='Monetary'),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "\n",
    "#set characteristics\n",
    "fig.update_layout(height=350,\n",
    "                  width=1170,\n",
    "                  showlegend=False,\n",
    "                  title_text='Distribution of Recency, Frequency, and Monetary',\n",
    "                  \n",
    ")\n",
    "\n",
    "#fig.show()\n",
    "\n",
    "#export the graph\n",
    "fig.write_html('plotly_graphs/2.3.1_box_rfm_features.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcZGhGc-ULfE"
   },
   "source": [
    "Что интересного здесь можно увидеть? Есть клиенты с аномально большим количеством сделанных заказов (более 100 штук), а также клиенты, общая стоимость заказов которых превышает 190 тысяч фунтов стерлингов.\n",
    "\n",
    "Чем это плохо? Выбросы могут отрицательно сказаться на результатах работы методов кластеризации, неустойчивых к ним, например алгоритма KMeans, поэтому хотелось бы от них избавиться. Однако терять много ценных данных о клиентах тоже не хочется, поэтому ограничимся верхней границей соответствующей квантили уровня 0.95. Таким образом, мы удалим данные тех клиентов, для которых значение параметра Frequency или параметра Monetary выше, чем у 95 % клиентов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the limits for frequency and monetary\n",
    "freq_bound = np.quantile(rfm_table['Frequency'], 0.95)\n",
    "monetary_bound = np.quantile(rfm_table['Monetary'], 0.95)\n",
    "\n",
    "#set the limits\n",
    "rfm_table = rfm_table[~((rfm_table['Frequency'] > freq_bound) | (rfm_table['Monetary'] > monetary_bound))]\n",
    "rfm_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create side-by-side plots\n",
    "fig = make_subplots(rows=1,\n",
    "                    cols=3)\n",
    "\n",
    "#first graph: recency\n",
    "fig.add_trace(\n",
    "    go.Box(x=rfm_table.iloc[:, 0].values,\n",
    "           name='Recency'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "#second graph: frequency\n",
    "fig.add_trace(\n",
    "    go.Box(x=rfm_table.iloc[:, 1].values,\n",
    "           name='Frequency'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "#third graph: monetary\n",
    "fig.add_trace(\n",
    "    go.Box(x=rfm_table.iloc[:, 2].values,\n",
    "           name='Monetary'),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "\n",
    "#set characteristics\n",
    "fig.update_layout(height=350,\n",
    "                  width=1170,\n",
    "                  showlegend=False,\n",
    "                  title_text='Distribution of Recency, Frequency, and Monetary',\n",
    "                  \n",
    ")\n",
    "\n",
    "#fig.show()\n",
    "\n",
    "#export the graph\n",
    "fig.write_html('plotly_graphs/2.3.2_box_rfm_features.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** \\\n",
    "После удаления выбросов графики имеют характерную форму, но выбросы все еще присутствуют."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 3D scatter plot\n",
    "fig = px.scatter_3d(rfm_table,\n",
    "                    x='Recency',\n",
    "                    y='Frequency',\n",
    "                    z='Monetary',\n",
    "                    color='Monetary',\n",
    "                    size='Monetary'\n",
    "                    )\n",
    "\n",
    "#set characteristics\n",
    "fig.update_layout(height=750,\n",
    "                  width=900,\n",
    "                  showlegend=False,\n",
    "                  title_text='Recency, Frequency, and Monetary',                  \n",
    ")\n",
    "\n",
    "#fig.show()\n",
    "\n",
    "#export the graph\n",
    "fig.write_html('plotly_graphs/3.1_3d_scatter_rfm_features.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** \\\n",
    "По массе точек сложно сказать, сколько кластеров необходимо — скорее даже кажется, нужно проанализировать данные через несколько моделей кластеризации для принятия решения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A08Te5c8ULfG"
   },
   "source": [
    "### **3. Моделирование и Оценка Качества Моделей**\n",
    "#### Обучение без Учителя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saTbceTWULfG"
   },
   "source": [
    "#### **3.1. Кластеризация на основе RFM-Характеристик**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swJGE09NULfG"
   },
   "source": [
    "Чтобы результаты кластеризации было удобнее интерпретировать, вы можете уменьшить размерность исходных признаков до двух компонент.\n",
    "\n",
    "**Подсказка.** Чтобы методы понижения размерности работали стабильно, данные необходимо стандартизировать/нормализовать. Для удобства оберните эти шаги по предобработке данных в pipeline.\n",
    "\n",
    "Произведите предобработку исходных данных. На основе RFM-признаков кластеризуйте клиентов онлайн-магазина подарков с помощью известных вам методов (используйте минимум три метода).\n",
    "\n",
    "Подберите оптимальное количество кластеров для выбранных методов с помощью коэффициента силуэта, перебирая возможные значения от 3 до 10 включительно (большее или меньшее количество кластеров будет нерелеватно для маркетинга). \n",
    "\n",
    "Выберите алгоритм с наибольшим коэффициентом силуэта, сделайте предсказание меток кластеров.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the scaler\n",
    "s_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "#standardize the data\n",
    "rfm_table_scaled = s_scaler.fit_transform(rfm_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1.0. Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "нужно использовать снижение размерности, чтобы упростить большой набор данных до меньшего, при этом сохранить значимые закономерности и тенденции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the object class\n",
    "pca = decomposition.PCA()\n",
    "\n",
    "#model learning on standardised data set\n",
    "pca.fit_transform(rfm_table_scaled)\n",
    "\n",
    "#explained variance ratio on the number of features\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the % of data covered by the combination of features\n",
    "plt.plot(range(1, 4),\n",
    "         pca.explained_variance_ratio_.cumsum(),\n",
    "         marker='o',\n",
    "         linestyle='--')\n",
    "plt.title('Explained Variance by Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению нельзя выбрать 1,5 компонента, чтобы покрыть 80% данных для стандартного решения, поэтому нужно использовать количество компонентов = 2. \\\n",
    "2 компонента описывает >90% данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "\n",
    "#convert the dimensionality reduction into a pipeline\n",
    "pipeline = pipeline.Pipeline([\n",
    "    ('scaler', preprocessing.StandardScaler()), \n",
    "    ('pca', decomposition.PCA(n_components=2))\n",
    "])\n",
    "\n",
    "pca_model = pipeline.fit_transform(rfm_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the space of principal components after decomposition\n",
    "pca_processed = pd.DataFrame(pca_model,\n",
    "                             columns=['axis-1',\n",
    "                                      'axis-2'])\n",
    "\n",
    "#create the scatterplot\n",
    "sns.scatterplot(data=pca_processed,\n",
    "                x='axis-1',\n",
    "                y='axis-2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1.1. K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Внутренние Меры*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Silhouette Score** \\\n",
    "compare the change in silhouette score for standardised data set with all features and dimensionally reduced data set with 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the change in silhouette score\n",
    "clusters_by_silhouette('k-means', rfm_table_scaled, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the change in silhouette score\n",
    "clusters_by_silhouette('k-means', pca_processed, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'All Features, Best Result: {best_result_by_silhouette('k-means', rfm_table_scaled, 3, 11)}')\n",
    "print(f'Reduced Features, Best Result: {best_result_by_silhouette('k-means', pca_processed, 3, 11)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** \\\n",
    "Чем выше оценка, тем лучше, таким образом, оценка предполагает наилучшее количество кластеров = 3. \\\n",
    "Набор данных с уменьшенной размерностью показывает лучшие результаты для модели K-Means: 0.523 > 0.474."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calinski-Harabasz Index** \\\n",
    "compare the change in calinski-harabasz score for standardised data set with all features and dimensionally reduced data set with 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_by_calinski_harabasz('k-means', rfm_table_scaled, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_by_calinski_harabasz('k-means', pca_processed, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'All Features, Best Result: {best_result_by_calinski_harabasz('k-means', rfm_table_scaled, 3, 11)}')\n",
    "print(f'Reduced Features, Best Result: {best_result_by_calinski_harabasz('k-means', pca_processed, 3, 11)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** \\\n",
    "Чем выше оценка, тем лучше, таким образом, оценка предполагает наилучшее количество кластеров = 6. \\\n",
    "Набор данных с уменьшенной размерностью показывает лучшие результаты для модели K-Means: 6984.539 > 4467.882."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Davies-Bouldin Index** \\\n",
    "compare the change in davies-bouldin score for standardised data set with all features and dimensionally reduced data set with 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_by_davies_bouldin('k-means', rfm_table_scaled, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_by_davies_bouldin('k-means', pca_processed, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'All Features, Best Result: {best_result_by_davies_bouldin('k-means', rfm_table_scaled, 3, 11)}')\n",
    "print(f'Reduced Features, Best Result: {best_result_by_davies_bouldin('k-means', pca_processed, 3, 11)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** \\\n",
    "Чем ниже оценка, тем лучше, таким образом, оценка предполагает наилучшее количество кластеров = 3. \\\n",
    "Набор данных с уменьшенной размерностью показывает лучшие результаты для модели K-Means: 0.632 < 0.742."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means with Clusters suggested by Internal Measures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe to store model results\n",
    "compare_models_table = rfm_table.copy()\n",
    "\n",
    "k_means_final_model = cluster.KMeans(n_clusters=3,\n",
    "                                     init='k-means++',\n",
    "                                     random_state=42)\n",
    "\n",
    "#model learning\n",
    "k_means_final_model.fit(pca_processed)\n",
    "\n",
    "#list of clusters\n",
    "km_labels = k_means_final_model.labels_\n",
    "\n",
    "compare_models_table['KM-Labels'] = km_labels\n",
    "compare_models_table['KM-Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1.2. EM-Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Внутренние Меры*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Silhouette Score** \\\n",
    "compare the change in silhouette score for standardised data set with all features and dimensionally reduced data set with 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_by_silhouette('em-algorithm', rfm_table_scaled, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_by_silhouette('em-algorithm', pca_processed, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'All Features, Best Result: {best_result_by_silhouette('em-algorithm', rfm_table_scaled, 3, 11)}')\n",
    "print(f'Reduced Features, Best Result: {best_result_by_silhouette('em-algorithm', pca_processed, 3, 11)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** \\\n",
    "Чем выше оценка, тем лучше, таким образом, оценка предполагает наилучшее количество кластеров = 3. \\\n",
    "Набор данных с уменьшенной размерностью показывает лучшие результаты для модели EM-Algorithm: 0.432 > 0.268."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calinski-Harabasz Index** \\\n",
    "compare the change in calinski-harabasz score for standardised data set with all features and dimensionally reduced data set with 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_by_calinski_harabasz('em-algorithm', rfm_table_scaled, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_by_calinski_harabasz('em-algorithm', pca_processed, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'All Features, Best Result: {best_result_by_calinski_harabasz('em-algorithm', rfm_table_scaled, 3, 11)}')\n",
    "print(f'Reduced Features, Best Result: {best_result_by_calinski_harabasz('em-algorithm', pca_processed, 3, 11)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** \\\n",
    "Чем выше оценка, тем лучше, таким образом, оценка предполагает наилучшее количество кластеров = 3. \\\n",
    "Набор данных с уменьшенной размерностью показывает лучшие результаты для модели EM-Algorithm: 4328.155 > 2127.646."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Davies-Bouldin Index** \\\n",
    "compare the change in davies-bouldin score for standardised data set with all features and dimensionally reduced data set with 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_by_davies_bouldin('em-algorithm', rfm_table_scaled, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_by_davies_bouldin('em-algorithm', pca_processed, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'All Features, Best Result: {best_result_by_davies_bouldin('em-algorithm', rfm_table_scaled, 3, 11)}')\n",
    "print(f'Reduced Features, Best Result: {best_result_by_davies_bouldin('em-algorithm', pca_processed, 3, 11)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** \\\n",
    "Чем ниже оценка, тем лучше, таким образом, оценка предполагает наилучшее количество кластеров = 3. \\\n",
    "Набор данных с уменьшенной размерностью показывает лучшие результаты для модели EM-Algorithm: 0.723 < 1.224."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EM-Algorithm with Clusters suggested by Internal Measures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_final_model = mixture.GaussianMixture(n_components=3,\n",
    "                                         random_state=42)\n",
    "\n",
    "#model learning > list of clusters\n",
    "predictions = em_final_model.fit_predict(pca_processed)\n",
    "\n",
    "compare_models_table['EM-Labels'] = predictions\n",
    "compare_models_table['EM-Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1.3. Agglomerate Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Внутренние Меры*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Silhouette Score** \\\n",
    "compare the change in silhouette score for standardised data set with all features and dimensionally reduced data set with 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_by_silhouette('agglomerative_clustering', rfm_table_scaled, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_by_silhouette('agglomerative_clustering', pca_processed, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'All Features, Best Result: {best_result_by_silhouette('agglomerative_clustering', rfm_table_scaled, 3, 11)}')\n",
    "print(f'Reduced Features, Best Result: {best_result_by_silhouette('agglomerative_clustering', pca_processed, 3, 11)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** \\\n",
    "Чем выше оценка, тем лучше, таким образом, оценка предполагает наилучшее количество кластеров = 3 с linkage = average. \\\n",
    "Набор данных с уменьшенной размерностью показывает лучшие результаты для модели Agglomerative Clustering, но не значительно: 0.477 > 0.475."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calinski-Harabasz Index** \\\n",
    "compare the change in calinski-harabasz score for standardised data set with all features and dimensionally reduced data set with 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_by_calinski_harabasz('agglomerative_clustering', rfm_table_scaled, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_by_calinski_harabasz('agglomerative_clustering', pca_processed, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'All Features, Best Result: {best_result_by_calinski_harabasz('agglomerative_clustering', rfm_table_scaled, 3, 11)}')\n",
    "print(f'Reduced Features, Best Result: {best_result_by_calinski_harabasz('agglomerative_clustering', pca_processed, 3, 11)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** \\\n",
    "Чем выше оценка, тем лучше, таким образом, оценка предполагает наилучшее количество кластеров = 3 с linkage = ward. \\\n",
    "Набор данных с уменьшенной размерностью показывает лучшие результаты для модели Agglomerative Clustering: 5850.243 > 3814.718."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Davies-Bouldin Index** \\\n",
    "compare the change in davies-bouldin score for standardised data set with all features and dimensionally reduced data set with 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_by_davies_bouldin('agglomerative_clustering', rfm_table_scaled, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_by_davies_bouldin('agglomerative_clustering', pca_processed, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'All Features, Best Result: {best_result_by_davies_bouldin('agglomerative_clustering', rfm_table_scaled, 3, 11)}')\n",
    "print(f'Reduced Features, Best Result: {best_result_by_davies_bouldin('agglomerative_clustering', pca_processed, 3, 11)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** \\\n",
    "Чем ниже оценка, тем лучше, таким образом, оценка предполагает наилучшее количество кластеров = 4 с linkage = single. \\\n",
    "Набор данных со всеми компонентами показывает лучшие результаты для модели Agglomerative Clustering: 0.419 < 0.428."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Индекс Дэвиса-Боулдинга обычно выше для выпуклых кластеров, чем для других концепций кластеров, таких как кластеры на основе плотности, подобные тем, что получены из DBSCAN, поэтому для анализа будем опираться на первых два внутренних мер."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agglomerative Clustering with Clusters suggested by Internal Measures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggl_model_av = cluster.AgglomerativeClustering(n_clusters=6,\n",
    "                                                linkage='average')\n",
    "\n",
    "#model learning > list of clusters\n",
    "y_pred_av = aggl_model_av.fit_predict(pca_processed)\n",
    "\n",
    "compare_models_table['AC-Labels_Av'] = y_pred_av"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1.4. Density-Based Spatial Clustering of Applications with Noise (DBSCAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "вычислить k-nearest neighbors (рассчитать расстояние точки до ее k-го ближайшего соседа) для поиска оптимального значения eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the appropriate value of eps\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "#initialize the value of k for kNN <> MinPts\n",
    "MinPts = rfm_table.shape[1] * 2\n",
    "k = MinPts\n",
    "\n",
    "#add 1 to k to return distance to itself (1st column as 0)\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1).fit(rfm_table)\n",
    "\n",
    "#find the distances\n",
    "dist, ind = nbrs.kneighbors(rfm_table)\n",
    "\n",
    "#drop 1st column and sort the distances (ASC)\n",
    "k_dist = np.sort(dist[:, -1])\n",
    "\n",
    "fig = px.line(k_dist,\n",
    "              labels={'index': 'Distance Sorted Points',\n",
    "                      'value': f'{k}-Distance'\n",
    "              },\n",
    "              title='Nearest Neighbors Analysis')\n",
    "\n",
    "#set characteristics\n",
    "fig.update_layout(width=700,\n",
    "                  height=450,\n",
    "                  showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**: \\\n",
    "Перегиб на уровне 0.25-0.30, что указывает на хорошее значение для eps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe to store model results\n",
    "#dbscan_table = rfm_table.copy()\n",
    "\n",
    "dbscan = cluster.DBSCAN(eps=0.25,\n",
    "                        min_samples=MinPts).fit(rfm_table_scaled)\n",
    "\n",
    "dbscan_pca = cluster.DBSCAN(eps=0.25,\n",
    "                            min_samples=MinPts).fit(pca_processed)\n",
    "\n",
    "compare_models_table['DB-Labels_S'] = dbscan.labels_\n",
    "compare_models_table['DB-Labels_DR'] = dbscan_pca.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_models_table['DB-Labels_S'].nunique(), compare_models_table['DB-Labels_DR'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шумным образцам присваивается метка -1: \\\n",
    "35 кластеров + шум для данных со всеми признаками, и только 3 кластера + шум для набора данных с уменьшенной размерностью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dbscan_pca.labels_\n",
    "\n",
    "cl_number_for_dbscan = compare_models_table['DB-Labels_DR'].nunique()-1\n",
    "print(f'Clusters: {cl_number_for_dbscan}, Silhouette Score: {round(metrics.silhouette_score(pca_processed, y_pred), 3)}')\n",
    "print(f'Clusters: {cl_number_for_dbscan}, Calinksi-Harabasz Score: {round(metrics.calinski_harabasz_score(pca_processed, y_pred), 3)}')\n",
    "print(f'Clusters: {cl_number_for_dbscan}, Davies-Bouldin Score: {round(metrics.davies_bouldin_score(pca_processed, y_pred), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models_table.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2.0. t-Distributed Stochastic Neighbour Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEAL ANSWER?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_tsne = Pipeline([\n",
    "#     ('scaler', preprocessing.StandardScaler()), \n",
    "#     ('tsne', manifold.TSNE(perplexity=50, random_state=100))\n",
    "# ])\n",
    "# tsne = pipeline_tsne.fit_transform(rfm_table)\n",
    "# print('{:.2f}'.format(pipeline_tsne['tsne'].kl_divergence_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfm_table_processed = pd.DataFrame(tsne, columns=['axis-1', 'axis-2'])\n",
    "\n",
    "# fig = plt.figure(figsize=(12, 5))\n",
    "# sns.scatterplot(data=rfm_table_processed, x='axis-1', y='axis-2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find the best hyperparameters for t-sne model using optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna import Trial, study, samplers\n",
    "\n",
    "def optuna_tsne(trial):\n",
    "  #set hyperparameters\n",
    "  n_components = trial.suggest_categorical('n_components', [2])\n",
    "  perplexity = trial.suggest_categorical('perplexity', [5, 10, 20, 30, 40, 50])\n",
    "  init = trial.suggest_categorical('init', ['random', 'pca'])\n",
    "  learning_rate = trial.suggest_categorical('learning_rate', [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600])\n",
    "  early_exaggeration = trial.suggest_categorical('early_exaggeration', [1, 2, 3, 4, 5, 6, 7, 8])\n",
    "  \n",
    "  #use the combinations for model build\n",
    "  model = manifold.TSNE(n_components=n_components,\n",
    "                        perplexity=perplexity,\n",
    "                        learning_rate=learning_rate,\n",
    "                        init=init,\n",
    "                        early_exaggeration=early_exaggeration)\n",
    "  \n",
    "  tsne_results = model.fit(rfm_table_scaled)\n",
    "\n",
    "  return round(tsne_results.kl_divergence_, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#begin hyperparameters selection\n",
    "#create review object\n",
    "study_optuna = optuna.create_study(study_name='t-distributed stochastic neighbor embedding',\n",
    "                                   direction='minimize')\n",
    "\n",
    "#search for the best combination\n",
    "study_optuna.optimize(optuna_tsne,\n",
    "                      n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'best value: {study_optuna.best_value:.3f}')\n",
    "print(f'best hyperparameters for t-sne: {study_optuna.best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model learning on standardised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the metrics for test data\n",
    "model_opt_tsne = manifold.TSNE(**study_optuna.best_params,\n",
    "                               random_state=42)\n",
    "\n",
    "#model learning\n",
    "tsne = model_opt_tsne.fit_transform(rfm_table_scaled)\n",
    "\n",
    "#make a prediction\n",
    "model_opt_tsne.kl_divergence_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the dataframe to store the results\n",
    "rfm_table_processed = pd.DataFrame(tsne,\n",
    "                                   columns=['axis-1', 'axis-2'])\n",
    "\n",
    "#create the scatterplot to visualize the clusters\n",
    "sns.scatterplot(data=rfm_table_processed,\n",
    "                x='axis-1', y='axis-2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE сгруппировал наиболее похожие объекты в подобие кластеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means with t-sne by internal measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result_by_silhouette('k-means', tsne, 3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result_by_calinski_harabasz('k-means', tsne, 3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result_by_davies_bouldin('k-means', tsne, 3, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "em-algorithm with t-sne by internal measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result_by_silhouette('em-algorithm', tsne, 3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result_by_calinski_harabasz('em-algorithm', tsne, 3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result_by_davies_bouldin('em-algorithm', tsne, 3, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agglomerative clustering with t-sne by internal measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result_by_silhouette('agglomerative_clustering', tsne, 3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result_by_calinski_harabasz('agglomerative_clustering', tsne, 3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result_by_davies_bouldin('agglomerative_clustering', tsne, 3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = cluster.KMeans(n_clusters=7)\n",
    "tree.fit(tsne)\n",
    "\n",
    "pd.DataFrame((np.unique(tree.labels_,\n",
    "                        return_counts=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=rfm_table_processed,\n",
    "                x='axis-1', y='axis-2',\n",
    "                hue=tree.labels_.astype('str'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZlG9NGeULfH"
   },
   "source": [
    "#### **3.2. Интерпретация Результатов Кластеризации**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDCSx88tULfH"
   },
   "source": [
    "Перейдём к интерпретации полученных кластеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aYSlmTbULfH"
   },
   "source": [
    "##### 3.2.1. Визуализация Кластеров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmzRFd2jULfH"
   },
   "source": [
    "Визуализируйте результаты в виде 3D-диаграммы с осями Recency, Frequency и Monetary. Проанализируйте полученную диаграмму и попробуйте понять, какие кластеры у вас получились."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_table['Cluster'] = tree.labels_\n",
    "rfm_clusters = rfm_table.groupby('Cluster').mean().round(0)\n",
    "rfm_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "MUg19zX-ULfH"
   },
   "outputs": [],
   "source": [
    "#create 3D scatter plot\n",
    "fig = px.scatter_3d(rfm_table,\n",
    "                    x='Recency',\n",
    "                    y='Frequency',\n",
    "                    z='Monetary',\n",
    "                    color='Cluster',\n",
    "                    size='Cluster'\n",
    "                    )\n",
    "\n",
    "#set characteristics\n",
    "fig.update_layout(height=750,\n",
    "                  width=900,\n",
    "                  showlegend=False,\n",
    "                  title_text='Recency, Frequency, and Monetary',                  \n",
    ")\n",
    "\n",
    "#fig.show()\n",
    "\n",
    "#export the graph\n",
    "fig.write_html('plotly_graphs/3.2_3d_scatter_rfm_features_with_clusters.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhjTl98iULfH"
   },
   "source": [
    "##### 3.2.2. Построение Профиля Кластеров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t54P-tw3ULfH"
   },
   "source": [
    "Далее составьте так называемый профиль кластеров. Для этого вам необходимо вернуться от декомпозированных данных (если вы производили понижение размерности) к RFM-таблице (очищенной от выбросов).\n",
    "\n",
    "Сгруппируйте RFM-таблицу по полученным кластерам и рассчитайте среднее по каждому из признаков.\n",
    "\n",
    "Чтобы результаты было проще интерпретировать, давайте познакомимся с одним из способов визуализации профиля кластеров — **Radar Chart** (полярная диаграмма, или диаграмма паутины). Это графическое представление значений нескольких эквивалентных категорий в форме паутины.\n",
    "\n",
    "Radar Chart часто используется при определении профиля кластеров. На концах паутины откладываются оси, соответствующие признакам, описывающим объекты. На каждой из осей для каждого кластера откладываются средние значения соответствующих характеристик. Соединив точки по осям, мы получаем многоугольник. \n",
    "\n",
    "Пример полярной диаграммы для задачи кластеризации учеников по интересам:\n",
    "\n",
    "<img src=https://www.datanovia.com/en/wp-content/uploads/2020/12/radar-chart-in-r-customized-fmstb-radar-chart-1.png width=500>\n",
    "\n",
    "На этой диаграмме мы видим визуализацию признаков для одного из кластеров. Видно, что ученики, принадлежащие этому кластеру, в большей степени увлекаются музыкой (Music), а в меньшей — программированием (Programm).\n",
    "\n",
    "В модуле `graph_objects` библиотеки `plotly` есть встроенная функция `Scatterpolar`, которая позволяет построить полярную диаграмму. На основе этой функции мы подготовили для вас функцию `plot_cluster_profile()`, которая позволяет визуализировать профиль каждого из кластеров в виде полярной диаграммы. У неё есть два параметра: `grouped_data` — сгруппированные по кластерам характеристики объектов (клиентов), `n_clusters` — количество кластеров.\n",
    "\n",
    "Главное условие использования полярной диаграммы — все признаки должны быть приведены к единому масштабу с помощью нормализации, где 1 будет означать максимум, а 0 — минимум. Шаг с нормализацией мы также добавили в функцию `plot_cluster_profile()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_profile(rfm_clusters, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atfZXLINULfI"
   },
   "source": [
    "Теперь у вас есть удобный инструмент для анализа профиля кластеров. Воспользуйтесь им, чтобы проинтерпретировать результаты, полученные на предыдущем шаге."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINISH OFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**: \\\n",
    "Группа **БЕДНЫЕ НОВИЧКИ** не делают значительного вклада в данные и имеют одинаково малые показатели по всем 3 осям. Группа **БЕДНЫЕ СПЯЩИЕ** делали заказы очень давно, но не часто и не принесли значительной прибыли магазину. А группа **БОГАТЫЕ ЛОЯЛЬНЫЕ** делают заказы часто и тратят много денег. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Моделирование и Оценка Качества Моделей**\n",
    "#### Обучение с Учителем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the features\n",
    "X = rfm_table.drop(columns='Cluster')\n",
    "y = rfm_table['Cluster']\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y,\n",
    "                                                                    test_size=0.3,\n",
    "                                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the clusters\n",
    "cluster_report = ['Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5', 'Cluster 6', 'Cluster 7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.1. Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the Optuna method to look for best hyper parameters\n",
    "def optuna_rf(trial):\n",
    "  #set hyperparameters\n",
    "  n_estimators = trial.suggest_categorical('n_estimators', [50, 100, 150, 200])\n",
    "  max_depth = trial.suggest_categorical('max_depth', [3, 5, 7])\n",
    "  min_samples_leaf = trial.suggest_categorical('min_samples_leaf', [2, 3, 4])\n",
    "  criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "  max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "  \n",
    "  #use the combinations for model build\n",
    "  model = ensemble.RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                          max_depth=max_depth,\n",
    "                                          min_samples_leaf=min_samples_leaf,\n",
    "                                          criterion=criterion,\n",
    "                                          max_features=max_features,\n",
    "                                          random_state=42)\n",
    "  \n",
    "  #model learning through cross-validation\n",
    "  score = model_selection.cross_val_score(\n",
    "    model,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    cv=5,\n",
    "    scoring='f1_micro', \n",
    "    n_jobs=-1).mean()\n",
    "\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#begin hyperparameters selection\n",
    "#create review object\n",
    "study_optuna_rf = optuna.create_study(study_name='random forest classifier',\n",
    "                                       direction='maximize')\n",
    "\n",
    "#search for the best combination\n",
    "study_optuna_rf.optimize(optuna_rf,\n",
    "                         n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'best value: {round(study_optuna_rf.best_value, 3)}')\n",
    "print(f'best hyperparameters for random forest: {study_optuna_rf.best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the metrics for test data\n",
    "model_opt_rf = ensemble.RandomForestClassifier(**study_optuna_rf.best_params,\n",
    "                                               random_state=42,\n",
    "                                               )\n",
    "\n",
    "#model learning\n",
    "model_opt_rf.fit(X_train, y_train)\n",
    "\n",
    "#make a prediction\n",
    "y_train_pred_rf = model_opt_rf.predict(X_train)\n",
    "y_test_pred_rf = model_opt_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_train,\n",
    "                                    y_train_pred_rf,\n",
    "                                    target_names=cluster_report,\n",
    "                                    digits=5))\n",
    "\n",
    "print(metrics.classification_report(y_test,\n",
    "                                    y_test_pred_rf,\n",
    "                                    target_names=cluster_report,\n",
    "                                    digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3. Gradient Boosting Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the Optuna method to look for best hyper parameters\n",
    "def optuna_gb(trial):\n",
    "  #set hyperparameters\n",
    "  params = {\n",
    "    \"n_estimators\": trial.suggest_categorical('n_estimators', [50, 100, 150, 200]),\n",
    "    \"learning_rate\": trial.suggest_categorical('learning_rate', [0.001, 0.01, 0.05, 0.1, 0.15, 0.2]),    \n",
    "    \"max_depth\": trial.suggest_categorical('max_depth', [3, 5, 7]),\n",
    "    \"max_features\": trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "    \"min_samples_leaf\": trial.suggest_categorical('min_samples_leaf', [2, 3, 4]),\n",
    "    \"loss\" : \"log_loss\",\n",
    "    \"random_state\": 42,\n",
    "    }\n",
    "\n",
    "  #use the combinations for model build\n",
    "  model = ensemble.GradientBoostingClassifier(**params)\n",
    "  \n",
    "  #model learning through cross-validation\n",
    "  score = model_selection.cross_val_score(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring='f1_micro',\n",
    "    n_jobs=-1).mean()\n",
    "\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#begin hyperparameters selection\n",
    "#create review object\n",
    "study_optuna_gb = optuna.create_study(study_name='gradient boosting classifier',\n",
    "                                   direction='maximize')\n",
    "\n",
    "#search for the best combination\n",
    "study_optuna_gb.optimize(optuna_gb,\n",
    "                         n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'best value: {round(study_optuna_gb.best_value, 3)}')\n",
    "print(f'best hyperparameters for random forest: {study_optuna_gb.best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the metrics for test data\n",
    "model_opt_gb = ensemble.GradientBoostingClassifier(**study_optuna_gb.best_params,\n",
    "                                                   random_state=42,\n",
    "                                                   )\n",
    "\n",
    "#model learning\n",
    "model_opt_gb.fit(X_train, y_train)\n",
    "\n",
    "#make a prediction\n",
    "y_train_pred_gb = model_opt_gb.predict(X_train)\n",
    "y_test_pred_gb = model_opt_gb.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_train,\n",
    "                                    y_train_pred_gb,\n",
    "                                    target_names=cluster_report,\n",
    "                                    digits=5))\n",
    "\n",
    "print(metrics.classification_report(y_test,\n",
    "                                    y_test_pred_gb,\n",
    "                                    target_names=cluster_report,\n",
    "                                    digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.1. Кластеризация на основе RFM-Характеристик**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2. Интерпретация Результатов Кластеризации**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wMXSTTDULfI"
   },
   "source": [
    "### 5. **Выводы и Оформление Работы**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINISH OFF"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
